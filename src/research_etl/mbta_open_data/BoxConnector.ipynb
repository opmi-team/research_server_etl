{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3c9d71-81d0-4f7c-9ee6-6e8afb5f384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from boxsdk import JWTAuth, Client\n",
    "import io\n",
    "import os\n",
    "import polars as pl\n",
    "import psycopg2\n",
    "\n",
    "# Authentication and Client Setup\n",
    "def authenticate_box():\n",
    "    config = JWTAuth.from_settings_file(\"Token/BOX_TOKEN.json\")\n",
    "    client = Client(config)\n",
    "    user = client.user().get()\n",
    "    print(f\"Authenticated as {user.name}\")\n",
    "    return client\n",
    "\n",
    "# Search for all CSV files in the folder\n",
    "def find_csv_files(client, folder_id='287803137437'):\n",
    "    csv_files = []\n",
    "    items = client.folder(folder_id).get_items(limit=100)\n",
    "    for item in items:\n",
    "        if item.type == 'file' and item.name.endswith('.csv'):\n",
    "            print(f'CSV File Found: {item.name}')\n",
    "            csv_files.append(item)\n",
    "    if not csv_files:\n",
    "        print(\"No CSV files found.\")\n",
    "    return csv_files\n",
    "\n",
    "# Download and load file into a Polars DataFrame\n",
    "def download_csv_file(file_item):\n",
    "    if file_item:\n",
    "        file_stream = io.BytesIO()\n",
    "        file_item.download_to(file_stream)\n",
    "        file_stream.seek(0)\n",
    "        df = pl.read_csv(file_stream)\n",
    "        return df\n",
    "    return None\n",
    "\n",
    "# Check if table exists in PostgreSQL\n",
    "def check_table_exists(cursor, schema, table):\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT EXISTS (\n",
    "            SELECT 1 \n",
    "            FROM information_schema.tables \n",
    "            WHERE table_schema = %s AND table_name = %s\n",
    "        );\n",
    "    \"\"\", (schema, table))\n",
    "    return cursor.fetchone()[0]\n",
    "\n",
    "# Append DataFrame to PostgreSQL table without using execute_values\n",
    "def append_data_to_db(df, schema, table, db_params):\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_params)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        if not check_table_exists(cursor, schema, table):\n",
    "            print(f\"Table {schema}.{table} does not exist.\")\n",
    "            return False  # Exit early if the table doesn't exist\n",
    "\n",
    "        # Create a temporary table\n",
    "        temp_table = f\"{table}_temp\"\n",
    "        create_temp_table_query = f\"CREATE TEMP TABLE {temp_table} (LIKE {schema}.{table} INCLUDING ALL);\"\n",
    "        cursor.execute(create_temp_table_query)\n",
    "\n",
    "        # Prepare columns for insertion\n",
    "        columns = [f'\"{col}\"' for col in df.columns]\n",
    "        columns_str = ', '.join(columns)\n",
    "        values_placeholder = ', '.join(['%s'] * len(df.columns))\n",
    "        insert_temp_query = f\"INSERT INTO {temp_table} ({columns_str}) VALUES ({values_placeholder})\"\n",
    "\n",
    "        # Convert DataFrame to list of tuples\n",
    "        data_tuples = [tuple(row) for row in df.to_numpy()]\n",
    "\n",
    "        # Use executemany to insert data into the temporary table\n",
    "        cursor.executemany(insert_temp_query, data_tuples)\n",
    "\n",
    "        # Insert new records into the main table using NOT EXISTS\n",
    "        join_conditions = ' AND '.join([f'main.\"{col}\" = temp.\"{col}\"' for col in df.columns])\n",
    "        insert_main_query = f\"\"\"\n",
    "            INSERT INTO {schema}.{table} ({columns_str})\n",
    "            SELECT {columns_str} FROM {temp_table} temp\n",
    "            WHERE NOT EXISTS (\n",
    "                SELECT 1 FROM {schema}.{table} main\n",
    "                WHERE {join_conditions}\n",
    "            );\n",
    "        \"\"\"\n",
    "        cursor.execute(insert_main_query)\n",
    "\n",
    "        # Drop the temporary table\n",
    "        cursor.execute(f\"DROP TABLE {temp_table}\")\n",
    "\n",
    "        # Commit the transaction\n",
    "        conn.commit()\n",
    "        print(f\"Inserted new records into {schema}.{table}.\")\n",
    "        return True  # Indicate success\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while appending data to {schema}.{table}: {e}\")\n",
    "        return False  # Indicate failure\n",
    "\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "# Move file to 'imported' subfolder on Box\n",
    "def move_file_to_imported(client, file_item):\n",
    "    imported_folder_id = '287805162509'  # ID of the 'imported' subfolder\n",
    "    try:\n",
    "        # Move the file to the imported folder\n",
    "        imported_folder = client.folder(imported_folder_id)\n",
    "        file_item.move(imported_folder)\n",
    "        print(f\"File '{file_item.name}' moved to 'imported' folder.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to move file '{file_item.name}': {e}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    client = authenticate_box()\n",
    "    csv_files = find_csv_files(client)\n",
    "\n",
    "    if not csv_files:\n",
    "        print(\"No CSV files to process.\")\n",
    "    else:\n",
    "        db_params = {\n",
    "            'host': 'localhost',\n",
    "            'port': 5432,\n",
    "            'dbname': 'mbta_dw',\n",
    "            'user': 'opmi_etl',\n",
    "            'password': 'postgres' \n",
    "        }\n",
    "\n",
    "        for file_item in csv_files:  # Iterate through each found CSV file\n",
    "            df = download_csv_file(file_item)  # Download each file\n",
    "\n",
    "            if df is not None:\n",
    "                # Extract schema and table name from the file name\n",
    "                file_name = os.path.basename(file_item.name)  # Get the base file name\n",
    "                schema_table = file_name[:-4].split('.')  # Remove '.csv' and split\n",
    "                if len(schema_table) != 2:\n",
    "                    print(f\"File name '{file_item.name}' does not match expected format 'schema.table.csv'. Skipping.\")\n",
    "                    continue\n",
    "                schema, table = schema_table\n",
    "\n",
    "                # Append data to the database\n",
    "                if append_data_to_db(df, schema, table, db_params):\n",
    "                    move_file_to_imported(client, file_item)  # Move the file if successful\n",
    "                else:\n",
    "                    print(f\"Failed to append data for file '{file_item.name}'; file will not be moved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4006a881-130e-40c0-af2e-9f6922c51075",
   "metadata": {},
   "source": [
    "## SQL table creation (not used, table creation already in docker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869e28d6-b4e7-4ab4-8f15-a2e06b1aaa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from psycopg2 import sql\n",
    "\n",
    "# Database connection parameters\n",
    "db_params = {\n",
    "    'host': 'localhost',\n",
    "    'port': 5432,\n",
    "    'dbname': 'mbta_dw',\n",
    "    'user': 'opmi_etl',\n",
    "    'password': 'postgres'  # Replace with the actual password\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Connect to the PostgreSQL database\n",
    "    conn = psycopg2.connect(**db_params)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Create 'csat' table in 'surveys' schema\n",
    "    create_table_query = sql.SQL(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS surveys.csat (\n",
    "                survey_date DATE,\n",
    "                survey_name VARCHAR(255),\n",
    "                question_description TEXT,\n",
    "                response_total INTEGER,\n",
    "                response_1_text VARCHAR(255),\n",
    "                response_1_percent FLOAT,\n",
    "                response_2_text VARCHAR(255),\n",
    "                response_2_percent FLOAT,\n",
    "                response_3_text VARCHAR(255),\n",
    "                response_3_percent FLOAT,\n",
    "                response_4_text VARCHAR(255),\n",
    "                response_4_percent FLOAT,\n",
    "                response_5_text VARCHAR(255),\n",
    "                response_5_percent FLOAT,\n",
    "                response_6_text VARCHAR(255),\n",
    "                response_6_percent FLOAT,\n",
    "                response_7_text VARCHAR(255),\n",
    "                response_7_percent FLOAT\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    # Execute the create table query\n",
    "    cursor.execute(create_table_query)\n",
    "\n",
    "    # Commit the changes\n",
    "    conn.commit()\n",
    "\n",
    "    print(\"Table 'csat' created successfully in 'surveys' schema.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error while connecting to PostgreSQL or creating table:\", e)\n",
    "\n",
    "finally:\n",
    "    # Close the database connection\n",
    "    if cursor:\n",
    "        cursor.close()\n",
    "    if conn:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b3ae46-5efc-4511-836c-0f6ae6428861",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
